{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"v2_Assignment_Codes\")  # Add the folder to the search path\n",
    "\n",
    "#load data\n",
    "from v2_data import get_fixed_data\n",
    "from PriceProcess import price_model\n",
    "from WindProcess import wind_model\n",
    "from utils import generate_time_series,generate_experiment_series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pyomo.environ import *\n",
    "from mdp import check_feasibility,sim_MDP_exp, sim_MDP, generate_scenarios\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams.update({'font.size': 13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_fixed_data()\n",
    "T = data['num_timeslots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure we always use the same time series for all tasks and experiments\n",
    "prices,winds = generate_experiment_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction():\n",
    "    def __init__(self, T, state_dim):\n",
    "        self.T = T\n",
    "        self.state_dim = state_dim\n",
    "        self.weights = np.ones((T, state_dim + 1))  # +1 for the bias term\n",
    "        \n",
    "    def compute_value_explicit(self, t, state):\n",
    "        # Append 1 for the bias term to the state\n",
    "        if t >= T:\n",
    "            return 0\n",
    "        state_with_bias = state + [1]\n",
    "        value = 0\n",
    "        for j in range(len(state_with_bias)):\n",
    "            value += state_with_bias[j] * self.weights[t, j].item()\n",
    "        return value\n",
    "    \n",
    "    def compute_value(self, t, states):\n",
    "        if t >= T:\n",
    "            return np.zeros(states.shape[0])\n",
    "        # Append 1 for the bias term to each state\n",
    "        states_with_bias = np.hstack((states, np.ones((states.shape[0], 1))))\n",
    "        return np.dot(states_with_bias, self.weights[t])\n",
    "\n",
    "    def update(self, t, states, target_values):\n",
    "        if t >= T:\n",
    "            return \n",
    "        # Append 1 for the bias term to each state\n",
    "        states_with_bias = np.hstack((states, np.ones((states.shape[0], 1))))\n",
    "        # Solve the least squares problem to find the optimal weights\n",
    "        self.weights[t], _, _, _ = np.linalg.lstsq(states_with_bias, target_values, rcond=None)\n",
    "    \n",
    "    def squared_error(self, t, states, target_values):\n",
    "        # Compute the squared error\n",
    "        predicted_values = self.compute_value(t, states)\n",
    "        return np.mean((predicted_values - target_values) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to sample representative pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_representative_state_pairs(I):\n",
    "    T = data['num_timeslots']\n",
    "    state_pairs = np.zeros((T,I,7)) # seven state variables\n",
    "    for i in range(I):\n",
    "        # sample exogenous state variables\n",
    "        # TODO : should varying initial coniditions be used?\n",
    "        price, wind = generate_time_series(T)\n",
    "        # sample endogenous state variables\n",
    "        h = np.random.uniform(0, data['hydrogen_capacity'], T)\n",
    "        e_on = np.random.choice([0, 1], T)\n",
    "        for t in range(T):\n",
    "            state = [t, h[t], e_on[t-1] if t > 0 else 0, wind[t], wind[t-1] if t > 0 else data['wind_power_previous'], price[t], price[t-1] if t > 0 else data['price_previous']]\n",
    "            state_pairs[t, i] = state\n",
    "    return state_pairs\n",
    "state_pairs = sample_representative_state_pairs(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear program to perform value function minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_minimization(V: ValueFunction,t,state_cur,scenarios, gamma,print_result=False): \n",
    "\n",
    "    t, h, e_on_tm1, wind, wind_previous, price, price_previous = state_cur\n",
    "\n",
    "    # Create a model\n",
    "    model = ConcreteModel()\n",
    "    # Declare indexed variable for the price\n",
    "    model.p_grid = Var(within=NonNegativeReals,name='p_grid')\n",
    "    model.e_h2p = Var(within=NonNegativeReals,name='e_h2p')\n",
    "    model.e_p2h = Var(within=NonNegativeReals,name='e_p2h')\n",
    "    model.e_on = Var(within=Binary,name='e_on')\n",
    "    \n",
    "    # declare the new state\n",
    "    model.next_e_on = Var(within=Binary,name='new_e_on')\n",
    "    model.next_h = Var(within=NonNegativeReals,bounds=(0,data['hydrogen_capacity']),name='new_h')\n",
    "\n",
    "    # Objective function\n",
    "    def objective_rule(model):\n",
    "        \n",
    "        expected_next_value = 0\n",
    "        for scenario in scenarios:\n",
    "            scenario_state =  [t+1, model.next_h, model.next_e_on, scenario.wind, wind, scenario.price, price]\n",
    "            expected_next_value += V.compute_value_explicit(int(t)+1,scenario_state)\n",
    "        expected_next_value /= len(scenarios) \n",
    "        \n",
    "        return price * model.p_grid + data['electrolyzer_cost']*model.e_on + gamma * expected_next_value\n",
    "\n",
    "    model.profit = Objective(rule=objective_rule, sense=minimize)\n",
    "    model.DemandConstraint = Constraint(rule=lambda model: model.p_grid + wind + data['conversion_h2p']*model.e_h2p - model.e_p2h >= data['demand_schedule'][int(t)])\n",
    "\n",
    "    # contraints\n",
    "\n",
    "    model.h_contraint = Constraint(expr=lambda model: model.next_h == h + data['conversion_p2h']*model.e_p2h-model.e_h2p)\n",
    "\n",
    "    model.p2h_constraint = Constraint(rule=lambda model: model.e_h2p <= h)\n",
    "    model.p2h_constraint2 = Constraint(rule=lambda model: data['conversion_h2p']*model.e_h2p <= data['h2p_max_rate'])\n",
    "\n",
    "    model.conversion_contraint = Constraint(rule=lambda model: data['conversion_p2h'] * model.e_p2h <= data['p2h_max_rate']*e_on_tm1)\n",
    "\n",
    "    model.e_on_constraint = Constraint(rule=lambda model: model.e_on == model.next_e_on)\n",
    "\n",
    "    # Create a solver\n",
    "    solver = SolverFactory('gurobi')  # Make sure Gurobi is installed and properly configured\n",
    "\n",
    "    # Solve the model\n",
    "    results = solver.solve(model, tee=False)\n",
    "    if print_result:\n",
    "        # Check if an optimal solution was found\n",
    "        if results.solver.termination_condition == TerminationCondition.optimal:\n",
    "            print(\"Optimal solution found\")\n",
    "            print(f\"profit: {value(model.profit)}\")\n",
    "            print(f\"p_grid: {value(model.p_grid)}\")\n",
    "            print(f\"e_h2p: {value(model.e_h2p)}\")\n",
    "            print(f\"e_p2h: {value(model.e_p2h)}\")\n",
    "            print(f\"e_on: {value(model.e_on)}\")\n",
    "        else:\n",
    "            print(\"No optimal solution found.\")\n",
    "    decision = (model.e_on.value,model.e_p2h.value,model.e_h2p.value,model.p_grid.value)\n",
    "    return decision,value(model.profit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Backward Value Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=23\n",
      "5027.166681186691\n",
      "174.5833808403135\n",
      "t=22\n",
      "4829.509806440117\n",
      "112.83540374151303\n",
      "t=21\n",
      "4527.189411880017\n",
      "117.47265070615003\n",
      "t=20\n",
      "4205.523445309924\n",
      "77.45289595975068\n",
      "t=19\n",
      "4342.916787727729\n",
      "56.93450909681083\n",
      "t=18\n",
      "4214.927225623123\n",
      "18.994428504946505\n",
      "t=17\n",
      "3981.2990424841405\n",
      "9.674165245025316\n",
      "t=16\n",
      "4042.3905816908787\n",
      "16.819865607356164\n",
      "t=15\n",
      "3844.7432310860368\n",
      "151.20918466269364\n",
      "t=14\n",
      "4097.799613877365\n",
      "31.290982194315077\n",
      "t=13\n",
      "3884.3279076836793\n",
      "170.83140617507564\n",
      "t=12\n",
      "4409.916904887554\n",
      "335.8312852074807\n",
      "t=11\n",
      "4591.670627747121\n",
      "230.91143972129635\n",
      "t=10\n",
      "5364.239132005265\n",
      "205.66097233470347\n",
      "t=9\n",
      "6422.3558001278025\n",
      "324.412727787766\n",
      "t=8\n",
      "7638.4027831857165\n",
      "346.17897742457984\n",
      "t=7\n",
      "9835.02920533717\n",
      "333.9784211993891\n",
      "t=6\n",
      "13355.963438738643\n",
      "305.4033858386513\n",
      "t=5\n",
      "18520.769759733015\n",
      "406.22019708894055\n",
      "t=4\n",
      "22414.433597053467\n",
      "564.7943713863401\n",
      "t=3\n",
      "19677.237620558502\n",
      "267.47443140971797\n",
      "t=2\n",
      "22959.804503082138\n",
      "118.00272135180433\n",
      "t=1\n",
      "25151.257311893994\n",
      "10.361441618863196\n",
      "t=0\n",
      "24444.004756583952\n",
      "4.184370676781917e-27\n"
     ]
    }
   ],
   "source": [
    "def backward_value_approx(V, state_pairs, K, data,gamma=0.9):\n",
    "    T = data['num_timeslots']\n",
    "    I = state_pairs.shape[1]\n",
    "    for t in range(T-1, -1, -1):\n",
    "        print(f\"t={t}\")\n",
    "        value_targets = np.zeros(I)\n",
    "        # go trough state pairs\n",
    "        for i in range(I):\n",
    "            state = state_pairs[t, i]\n",
    "            _, h, e_on_tm1, wind, wind_previous, price, price_previous = state\n",
    "            scenarios, scenario_probs = generate_scenarios(wind, price, wind_previous, price_previous, 1, k=K, n_samples=K)\n",
    "            _,value_targets[i] = value_minimization(V, t, state, scenarios[0], gamma)\n",
    "        print(V.squared_error(t, state_pairs[t], value_targets))\n",
    "        V.update(t, state_pairs[t], value_targets)\n",
    "        print(V.squared_error(t, state_pairs[t], value_targets))        \n",
    "    return V\n",
    "\n",
    "\n",
    "state_pairs = sample_representative_state_pairs(100)\n",
    "V = backward_value_approx(ValueFunction(data['num_timeslots'],7),state_pairs,40,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating MDP: 100%|██████████| 10/10 [00:13<00:00,  1.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(429.0134423850388),\n",
       " [np.float64(23.862385671014),\n",
       "  np.float64(87.71661718473368),\n",
       "  np.float64(311.7918991088412),\n",
       "  np.float64(500.3320316028079),\n",
       "  np.float64(1035.7140267507584),\n",
       "  np.float64(142.83369483690421),\n",
       "  np.float64(616.9210524194654),\n",
       "  np.float64(864.8124024669761),\n",
       "  np.float64(533.694673587),\n",
       "  np.float64(172.45564022188677)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ADPPolicy():\n",
    "    def __init__(self, V, data,gamma = 0.9):\n",
    "        self.V = V\n",
    "        self.data = data\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, t, h, e_on, wind, wind_previous, price, price_previous,data):\n",
    "        scenarios, scenario_probs = generate_scenarios(wind, price, wind_previous, price_previous, 1)\n",
    "        decision, _ = value_minimization(self.V, t, [t,h,e_on,wind,wind_previous,price,price_previous], scenarios[0], self.gamma)\n",
    "        return decision\n",
    "\n",
    "\n",
    "adp_policy = ADPPolicy(V, data)\n",
    "sim_MDP(10,adp_policy,winds,prices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linprog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
