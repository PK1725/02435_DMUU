{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"v2_Assignment_Codes\")  # Add the folder to the search path\n",
    "\n",
    "#load data\n",
    "from v2_data import get_fixed_data\n",
    "from PriceProcess import price_model\n",
    "from WindProcess import wind_model\n",
    "from utils import generate_time_series,generate_experiment_series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pyomo.environ import *\n",
    "from mdp import check_feasibility,sim_MDP_exp, sim_MDP, generate_scenarios\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams.update({'font.size': 13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_fixed_data()\n",
    "T = data['num_timeslots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure we always use the same time series for all tasks and experiments\n",
    "prices,winds = generate_experiment_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction():\n",
    "    def __init__(self, T, state_dim):\n",
    "        self.T = T\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "        self.weights = np.ones((T, state_dim -1 + 1))  # -1 because state includes t, which we we told not to... +1 for the bias term\n",
    "        \n",
    "    def compute_value_explicit(self, t, state):\n",
    "        state = state[1:] # Exclude time from state\n",
    "        # Append 1 for the bias term to the state\n",
    "        if t >= T:\n",
    "            return 0\n",
    "        state_with_bias = state + [1]\n",
    "        value = 0\n",
    "        for j in range(len(state_with_bias)):\n",
    "            value += state_with_bias[j] * self.weights[t, j].item()\n",
    "        return value\n",
    "    \n",
    "    def compute_value(self, t, states):\n",
    "        states = states[:, 1:] # Exclude time from state\n",
    "        if t >= T:\n",
    "            return np.zeros(states.shape[0])\n",
    "        # Append 1 for the bias term to each state\n",
    "        states_with_bias = np.hstack((states, np.ones((states.shape[0], 1))))\n",
    "        return np.dot(states_with_bias, self.weights[t])\n",
    "\n",
    "    def update(self, t, states, target_values):\n",
    "        if t >= T:\n",
    "            return \n",
    "        states = states[:,1:] # Exclude time from state\n",
    "        # Append 1 for the bias term to each state\n",
    "        states_with_bias = np.hstack((states, np.ones((states.shape[0], 1))))\n",
    "        # Solve the least squares problem to find the optimal weights\n",
    "        self.weights[t], _, _, _ = np.linalg.lstsq(states_with_bias, target_values, rcond=None)\n",
    "    \n",
    "    def squared_error(self, t, states, target_values):\n",
    "        # Compute the squared error\n",
    "        predicted_values = self.compute_value(t, states)\n",
    "        return np.mean((predicted_values - target_values) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to sample representative pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_representative_state_pairs(I):\n",
    "    T = data['num_timeslots']\n",
    "    state_pairs = np.zeros((T,I,7)) # seven state variables\n",
    "    for i in range(I):\n",
    "        # sample exogenous state variables\n",
    "        # We always use the same initial coniditions be used?\n",
    "        price, wind = generate_time_series(T)\n",
    "        # sample endogenous state variables\n",
    "        h = np.random.uniform(0, data['hydrogen_capacity'], T)\n",
    "        e_on = np.random.choice([0, 1], T)\n",
    "        for t in range(T):\n",
    "            state = [t, h[t], e_on[t-1] if t > 0 else 0, wind[t], wind[t-1] if t > 0 else data['wind_power_previous'], price[t], price[t-1] if t > 0 else data['price_previous']]\n",
    "            state_pairs[t, i] = state\n",
    "    return state_pairs\n",
    "state_pairs = sample_representative_state_pairs(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear program to perform value function minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_minimization(V: ValueFunction,t,state_cur,scenarios, gamma,print_result=False): \n",
    "\n",
    "    t, h, e_on_tm1, wind, wind_previous, price, price_previous = state_cur\n",
    "\n",
    "    # Create a model\n",
    "    model = ConcreteModel()\n",
    "    # Declare indexed variable for the price\n",
    "    model.p_grid = Var(within=NonNegativeReals,name='p_grid')\n",
    "    model.e_h2p = Var(within=NonNegativeReals,name='e_h2p')\n",
    "    model.e_p2h = Var(within=NonNegativeReals,name='e_p2h')\n",
    "    model.e_on = Var(within=Binary,name='e_on')\n",
    "    \n",
    "    # declare the new state\n",
    "    model.next_e_on = Var(within=Binary,name='new_e_on')\n",
    "    model.next_h = Var(within=NonNegativeReals,bounds=(0,data['hydrogen_capacity']),name='new_h')\n",
    "\n",
    "    # Objective function\n",
    "    def objective_rule(model):\n",
    "        \n",
    "        expected_next_value = 0\n",
    "        for scenario in scenarios:\n",
    "            scenario_state =  [t+1, model.next_h, model.next_e_on, scenario.wind, wind, scenario.price, price]\n",
    "            expected_next_value += V.compute_value_explicit(int(t)+1,scenario_state)\n",
    "        expected_next_value /= len(scenarios) \n",
    "        \n",
    "        return price * model.p_grid + data['electrolyzer_cost']*model.e_on + gamma * expected_next_value\n",
    "\n",
    "    model.profit = Objective(rule=objective_rule, sense=minimize)\n",
    "    model.DemandConstraint = Constraint(rule=lambda model: model.p_grid + wind + data['conversion_h2p']*model.e_h2p - model.e_p2h >= data['demand_schedule'][int(t)])\n",
    "\n",
    "    # contraints\n",
    "\n",
    "    model.h_contraint = Constraint(expr=lambda model: model.next_h == h + data['conversion_p2h']*model.e_p2h-model.e_h2p)\n",
    "\n",
    "    model.p2h_constraint = Constraint(rule=lambda model: model.e_h2p <= h)\n",
    "    model.p2h_constraint2 = Constraint(rule=lambda model: data['conversion_h2p']*model.e_h2p <= data['h2p_max_rate'])\n",
    "\n",
    "    model.conversion_contraint = Constraint(rule=lambda model: data['conversion_p2h'] * model.e_p2h <= data['p2h_max_rate']*e_on_tm1)\n",
    "\n",
    "    model.e_on_constraint = Constraint(rule=lambda model: model.e_on == model.next_e_on)\n",
    "\n",
    "    # Create a solver\n",
    "    solver = SolverFactory('gurobi')  # Make sure Gurobi is installed and properly configured\n",
    "\n",
    "    # Solve the model\n",
    "    results = solver.solve(model, tee=False)\n",
    "    if print_result:\n",
    "        # Check if an optimal solution was found\n",
    "        if results.solver.termination_condition == TerminationCondition.optimal:\n",
    "            print(\"Optimal solution found\")\n",
    "            print(f\"profit: {value(model.profit)}\")\n",
    "            print(f\"p_grid: {value(model.p_grid)}\")\n",
    "            print(f\"e_h2p: {value(model.e_h2p)}\")\n",
    "            print(f\"e_p2h: {value(model.e_p2h)}\")\n",
    "            print(f\"e_on: {value(model.e_on)}\")\n",
    "        else:\n",
    "            print(\"No optimal solution found.\")\n",
    "    decision = (model.e_on.value,model.e_p2h.value,model.e_h2p.value,model.p_grid.value)\n",
    "    return decision,value(model.profit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Backward Value Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=23\n",
      "2435.9808619897494\n",
      "174.5833808403135\n",
      "t=22\n",
      "2494.5301137082392\n",
      "127.23138073521663\n",
      "t=21\n",
      "2654.071672862699\n",
      "159.03562990297564\n",
      "t=20\n",
      "2480.7824050261816\n",
      "121.42424287595406\n",
      "t=19\n",
      "2638.7181173247764\n",
      "101.7474507728809\n",
      "t=18\n",
      "2600.985149174976\n",
      "54.92640568481513\n",
      "t=17\n",
      "2492.470836280641\n",
      "25.749105246033785\n",
      "t=16\n",
      "2363.8754895971138\n",
      "32.9011648905813\n",
      "t=15\n",
      "2334.19136373315\n",
      "176.08931484967223\n",
      "t=14\n",
      "2550.879582621088\n",
      "55.387340523503426\n",
      "t=13\n",
      "2328.854396352657\n",
      "214.78603519130007\n",
      "t=12\n",
      "2362.850222125641\n",
      "425.69894272858386\n",
      "t=11\n",
      "2715.928466954537\n",
      "291.09801331983937\n",
      "t=10\n",
      "3753.7897277716065\n",
      "372.8059015606709\n",
      "t=9\n",
      "4398.811658594933\n",
      "708.1441081374599\n",
      "t=8\n",
      "4246.363128423825\n",
      "877.232921379835\n",
      "t=7\n",
      "4483.259159169016\n",
      "672.2661026735078\n",
      "t=6\n",
      "5660.272820108888\n",
      "717.1769730367067\n",
      "t=5\n",
      "7255.90041500011\n",
      "728.4464875856896\n",
      "t=4\n",
      "8316.284401087496\n",
      "1297.5406679464934\n",
      "t=3\n",
      "6802.887880071553\n",
      "1151.8939603519173\n",
      "t=2\n",
      "4816.337347336239\n",
      "205.18659114773186\n",
      "t=1\n",
      "2895.5865598865926\n",
      "23.64560349990244\n",
      "t=0\n",
      "1472.6210247740494\n",
      "39.45294445870262\n"
     ]
    }
   ],
   "source": [
    "def backward_value_approx(V, state_pairs, K, data,gamma=0.9):\n",
    "    T = data['num_timeslots']\n",
    "    I = state_pairs.shape[1]\n",
    "    for t in range(T-1, -1, -1):\n",
    "        print(f\"t={t}\")\n",
    "        value_targets = np.zeros(I)\n",
    "        # go trough state pairs\n",
    "        for i in range(I):\n",
    "            state = state_pairs[t, i]\n",
    "            _, h, e_on_tm1, wind, wind_previous, price, price_previous = state\n",
    "            # we only need the \n",
    "            scenarios, scenario_probs = generate_scenarios(wind, price, wind_previous, price_previous, 1, k=K, n_samples=K)\n",
    "            _,value_targets[i] = value_minimization(V, t, state, scenarios[1], gamma)\n",
    "            \n",
    "        print(V.squared_error(t, state_pairs[t], value_targets))\n",
    "        V.update(t, state_pairs[t], value_targets)\n",
    "        print(V.squared_error(t, state_pairs[t], value_targets))        \n",
    "    return V\n",
    "\n",
    "\n",
    "state_pairs = sample_representative_state_pairs(100)\n",
    "V = backward_value_approx(ValueFunction(data['num_timeslots'],7),state_pairs,100,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating MDP: 100%|██████████| 10/10 [00:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(428.2049578319455,\n",
       " [24.862385671014,\n",
       "  88.71661718473368,\n",
       "  312.7918991088412,\n",
       "  485.2773173486601,\n",
       "  1036.7140267643358,\n",
       "  143.83369483690421,\n",
       "  617.9210524194654,\n",
       "  865.8124024933712,\n",
       "  534.694673587,\n",
       "  171.42550890512874])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ADPPolicy():\n",
    "    def __init__(self, V, data,gamma = 0.9):\n",
    "        self.V = V\n",
    "        self.data = data\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, t, h, e_on, wind, wind_previous, price, price_previous,data):\n",
    "        scenarios, scenario_probs = generate_scenarios(wind, price, wind_previous, price_previous, 1)\n",
    "        decision, _ = value_minimization(self.V, t, [t,h,e_on,wind,wind_previous,price,price_previous], scenarios[0], self.gamma)\n",
    "        return decision\n",
    "\n",
    "\n",
    "adp_policy = ADPPolicy(V, data)\n",
    "sim_MDP(10,adp_policy,winds,prices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linprog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
